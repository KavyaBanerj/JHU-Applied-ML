{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a251ddc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26c49135",
   "metadata": {},
   "source": [
    "1. [50 pts] In this assignment, we will use a priori analysis to find phrases, or interesting word\n",
    "patterns, in a novel.\n",
    "\n",
    "Note that you are free to use any a priori analytics and algorithm library in this assignment. Use the nltk library corpus gutenberg API and load the novel 'carroll-alice.txt', which is Alice in Wonderland by Lewis Carroll (although his real name was Charles Dodgson). There are 1703 sentences in the novel—which can be represented as 1703 transactions. Use any means you like to parse/extract words and save in a .csv format to be read by\n",
    "Weka framework, similar to the a priori Analysis module. (Hint: Feel free to use mlxtend\n",
    "library instead of Weka.)\n",
    "\n",
    "Hint: Removing stop words and symbols using regular expressions can be helpful:\n",
    "```python\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "Stop_words = stopwords.words('english')\n",
    "Sentences = gutenberg.sents('carroll-alice.txt')\n",
    "TermsSentences = []\n",
    "\n",
    "for terms in Sentences:\n",
    "    terms = [w for w in terms if w not in Stop_words]\n",
    "    terms = [w for w in terms if re.search(r'^[a-zA-Z]{2}', w) is not None]\n",
    "```\n",
    "\n",
    "If you chose to Weka, use FPGrowth and start with default parameters. Reduce lowerBoundMinSupport to reach to a sweet point for the support and avoid exploding the number of rules generated.\n",
    "Report interesting patterns.\n",
    "\n",
    "(Example: Some of the frequently occurring phrases are “Mock Turtle”, “White Rabbit”, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8dce161",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 72\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd115cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/kavyabanerjee/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kavyabanerjee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kavyabanerjee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d86f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Load Alice in Wonderland text\n",
    "sentences = gutenberg.sents('carroll-alice.txt')\n",
    "\n",
    "# Define stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Initialize list to hold the filtered sentences\n",
    "terms_sentences = []\n",
    "\n",
    "# Filter sentences to remove stop words and non-alphabetic terms\n",
    "preprocessed_sentences = []\n",
    "for sentence in sentences:\n",
    "    filtered_sentence = [word for word in sentence if word.lower() not in stop_words and re.search(r'^[a-zA-Z]{2,}$', word)]\n",
    "    if filtered_sentence:  # Ensure the sentence is not empty after filtering\n",
    "        preprocessed_sentences.append(filtered_sentence)\n",
    "\n",
    "# Convert the sentences into a one-hot encoded DataFrame\n",
    "encoder = TransactionEncoder()\n",
    "onehot = encoder.fit_transform(preprocessed_sentences)\n",
    "onehot_df = pd.DataFrame(onehot, columns=encoder.columns_)\n",
    "\n",
    "# Apply the Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(onehot_df, min_support=0.02, use_colnames=True)\n",
    "\n",
    "# Generate association rules from the frequent itemsets\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e4a2a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(little)</td>\n",
       "      <td>(Alice)</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>0.318584</td>\n",
       "      <td>1.360774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Alice)</td>\n",
       "      <td>(little)</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>1.360774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Alice)</td>\n",
       "      <td>(said)</td>\n",
       "      <td>0.098609</td>\n",
       "      <td>0.421189</td>\n",
       "      <td>1.550612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(said)</td>\n",
       "      <td>(Alice)</td>\n",
       "      <td>0.098609</td>\n",
       "      <td>0.363029</td>\n",
       "      <td>1.550612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Alice)</td>\n",
       "      <td>(thought)</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.149871</td>\n",
       "      <td>3.347790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(thought)</td>\n",
       "      <td>(Alice)</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>3.347790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(King)</td>\n",
       "      <td>(said)</td>\n",
       "      <td>0.025408</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>2.620739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(said)</td>\n",
       "      <td>(King)</td>\n",
       "      <td>0.025408</td>\n",
       "      <td>0.093541</td>\n",
       "      <td>2.620739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Mock)</td>\n",
       "      <td>(Turtle)</td>\n",
       "      <td>0.033878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Turtle)</td>\n",
       "      <td>(Mock)</td>\n",
       "      <td>0.033878</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>28.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  antecedents consequents   support  confidence       lift\n",
       "0    (little)     (Alice)  0.021779    0.318584   1.360774\n",
       "1     (Alice)    (little)  0.021779    0.093023   1.360774\n",
       "2     (Alice)      (said)  0.098609    0.421189   1.550612\n",
       "3      (said)     (Alice)  0.098609    0.363029   1.550612\n",
       "4     (Alice)   (thought)  0.035088    0.149871   3.347790\n",
       "5   (thought)     (Alice)  0.035088    0.783784   3.347790\n",
       "6      (King)      (said)  0.025408    0.711864   2.620739\n",
       "7      (said)      (King)  0.025408    0.093541   2.620739\n",
       "8      (Mock)    (Turtle)  0.033878    1.000000  28.500000\n",
       "9    (Turtle)      (Mock)  0.033878    0.965517  28.500000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules_df = pd.DataFrame(rules, columns = ['antecedents', 'consequents', 'support', 'confidence', 'lift'])\n",
    "rules_df\n",
    "\n",
    "# output_csv = 'preprocessed_sentences.csv'\n",
    "# rules_df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d6f04",
   "metadata": {},
   "source": [
    "2. [50 pts] In the lecture module, the class NeuralNetMLP implements a neural network with a single hidden layer. Make the necessary modifications to upgrade it to a 2-hidden layer neural network. Run it on the MNIST dataset and report its performance.\n",
    "(Hint: Raschka, Chapter 12)\n",
    "\n",
    "\n",
    "Modifications include:\n",
    "\n",
    "- Adding an additional weight matrix to represent the weights between the first and second hidden layers.\n",
    "- Modifying the _forward method to include computations through the second hidden layer.\n",
    "- Adjusting the backpropagation steps in the fit method to account for the gradients and updates through the additional hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd850da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows= 60000, columns= 784\n",
      "Rows= 10000, columns= 784\n"
     ]
    }
   ],
   "source": [
    "def get_acc(_y_test, _y_pred):\n",
    "    return (np.sum(_y_test == _y_pred)).astype(float) / _y_test.shape[0]\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    from numpy import fromfile, uint8\n",
    "    import os\n",
    "    import struct\n",
    "    \n",
    "    labels_path = os.path.join(path, '%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = fromfile(lbpath, dtype=uint8)\n",
    "        with open(images_path, 'rb') as imgpath:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\",imgpath.read(16))\n",
    "            images = fromfile(imgpath, dtype=uint8).reshape(len(labels), 784)\n",
    "            images = ((images / 255.) - .5) * 2\n",
    "    return images, labels\n",
    "\n",
    "X_train_mnist, y_train_mnist = load_mnist('../../Desktop/APML/Datasets/mnist/', kind='train')\n",
    "print(f'Rows= {X_train_mnist.shape[0]}, columns= {X_train_mnist.shape[1]}')\n",
    "\n",
    "X_test_mnist, y_test_mnist = load_mnist('../../Desktop/APML/Datasets/mnist/', kind='t10k')\n",
    "print(f'Rows= {X_test_mnist.shape[0]}, columns= {X_test_mnist.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a87ed90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP(object):\n",
    "\n",
    "    def __init__(self, n_hidden1=30, n_hidden2=30, epochs=100, eta=0.001, minibatch_size=1, seed=None):\n",
    "        self.random = np.random.RandomState(seed)  # used to randomize weights\n",
    "        self.n_hidden1 = n_hidden1  # size of the first hidden layer\n",
    "        self.n_hidden2 = n_hidden2  # size of the second hidden layer - New addition\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.minibatch_size = minibatch_size  # size of training batch\n",
    "        self.w_out, self.w_h1, self.w_h2 = None, None, None  # Initialize w_h2 for the second hidden layer\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(_y, _n_classes):  # one hot encode the input class y\n",
    "        onehot = np.zeros((_n_classes, _y.shape[0]))\n",
    "        for idx, val in enumerate(_y.astype(int)):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(_z):  # Sigmoid activation function\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(_z, -250, 250)))\n",
    "\n",
    "    def _forward(self, _X):\n",
    "        # First hidden layer\n",
    "        z_h1 = np.dot(_X, self.w_h1)\n",
    "        a_h1 = self.sigmoid(z_h1)\n",
    "        \n",
    "        # Second hidden layer - New addition\n",
    "        z_h2 = np.dot(a_h1, self.w_h2)  # Input for h2 is output of h1\n",
    "        a_h2 = self.sigmoid(z_h2)  # Activation for h2\n",
    "        \n",
    "        # Output layer\n",
    "        z_out = np.dot(a_h2, self.w_out)  # Input for out is output of h2\n",
    "        a_out = self.sigmoid(z_out)\n",
    "        return z_h1, a_h1, z_h2, a_h2, z_out, a_out  # Include z_h2 and a_h2 in the return statement\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cost(y_enc, output):  # Compute the cost\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0-y_enc) * np.log(1.0-output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        return cost\n",
    "\n",
    "    def predict(self, _X):\n",
    "        # Forward pass to predict\n",
    "        z_h1, a_h1, z_h2, a_h2, z_out, a_out = self._forward(_X)\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, _X_train, _y_train, _X_valid, _y_valid):\n",
    "        n_output = np.unique(_y_train).shape[0]  # number of class labels\n",
    "        n_features = _X_train.shape[1]\n",
    "        \n",
    "        # Initialize weights for all layers\n",
    "        self.w_h1 = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden1))\n",
    "        self.w_h2 = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden1, self.n_hidden2))  # New addition for the second hidden layer\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden2, n_output))\n",
    "\n",
    "        y_train_enc = self.onehot(_y_train, n_output)  # one-hot encode original y\n",
    "\n",
    "        for ei in range(self.epochs):  # Training epochs\n",
    "            indices = np.arange(_X_train.shape[0])\n",
    "            \n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                z_h1, a_h1, z_h2, a_h2, z_out, a_out = self._forward(_X_train[batch_idx])\n",
    "                \n",
    "                # Backpropagation\n",
    "                sigmoid_derivative_h2 = a_h2 * (1.0 - a_h2)  # Derivative for h2\n",
    "                \n",
    "                delta_out = a_out - y_train_enc[batch_idx]  # Output layer error\n",
    "                delta_h2 = np.dot(delta_out, self.w_out.T) * sigmoid_derivative_h2  # Error for h2\n",
    "                \n",
    "                sigmoid_derivative_h1 = a_h1 * (1.0 - a_h1)\n",
    "                delta_h1 = np.dot(delta_h2, self.w_h2.T) * sigmoid_derivative_h1  # Error for h1\n",
    "                \n",
    "                # Gradients for weight updates\n",
    "                grad_w_out = np.dot(a_h2.T, delta_out)\n",
    "                grad_w_h2 = np.dot(a_h1.T, delta_h2)  # New addition for second hidden layer\n",
    "                grad_w_h1 = np.dot(_X_train[batch_idx].T, delta_h1)\n",
    "                \n",
    "                # Update weights\n",
    "                self.w_out -= self.eta * grad_w_out\n",
    "                self.w_h2 -= self.eta * grad_w_h2  # Update for second hidden layer\n",
    "                self.w_h1 -= self.eta * grad_w_h1\n",
    "\n",
    "            # Evaluation after each epoch\n",
    "            z_h1, a_h1, z_h2, a_h2, z_out, a_out = self._forward(_X_train)\n",
    "            cost = self.compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            y_train_pred = self.predict(_X_train)\n",
    "            y_valid_pred = self.predict(_X_valid)\n",
    "            train_acc = ((np.sum(_y_train == y_train_pred)).astype(float) / _X_train.shape[0])\n",
    "            valid_acc = ((np.sum(_y_valid == y_valid_pred)).astype(float) / _X_valid.shape[0])\n",
    "            print('\\r%d/%d | Cost: %.2f ' '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
    "                  (ei+1, self.epochs, cost, train_acc*100, valid_acc*100), end='')\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "169f68fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 | Cost: 6944.08 | Train/Valid Acc.: 98.40%/96.26% Accuracy= 94.82%\n",
      "[[ 955    0    3    0    1    5    7    6    2    1]\n",
      " [   0 1114    5    1    0    2    2    3    8    0]\n",
      " [   6    3  974   16    8    1    7    5   12    0]\n",
      " [   3    0   13  956    0   20    0    9    7    2]\n",
      " [   1    2    8    0  931    1    9    1    4   25]\n",
      " [   8    1    0   15    3  837    8    5    8    7]\n",
      " [   8    1    1    2    3   14  920    0    9    0]\n",
      " [   2    7   15   10   12    1    0  963    1   17]\n",
      " [   7    2    9   19    7   14    8    5  896    7]\n",
      " [   1    9    1   13   18    6    2   11   12  936]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nn = NeuralNetMLP(n_hidden1=20, n_hidden2=15, epochs=300, eta=0.0005, minibatch_size=100, seed=1)\n",
    "nn.fit(X_train_mnist[:55000], y_train_mnist[:55000], X_train_mnist[55000:], y_train_mnist[55000:])\n",
    "\n",
    "y_pred = nn.predict(X_test_mnist)\n",
    "\n",
    "print(f'Accuracy= {get_acc(y_test_mnist, y_pred)*100:.2f}%')\n",
    "print(confusion_matrix(y_test_mnist, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68244c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
