{"cells":[{"cell_type":"markdown","source":["In this assignment, we will continue working on image classification using PyTorch and develop another model for the intel image dataset"],"metadata":{"id":"M_FOo_6MTLnx"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.datasets as dset\n","import torchvision.transforms as vtransforms\n","\n","# Set the GPU to device 0\n","gpu = torch.device('cuda:0')\n","\n","print(f'PyTorch version= {torch.__version__}')\n","print(f'torchvision version= {torchvision.__version__}')\n","print(f'CUDA available= {torch.cuda.is_available()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Go8CGnQhIKSv","executionInfo":{"status":"ok","timestamp":1714582848495,"user_tz":240,"elapsed":25127,"user":{"displayName":"Kavya Banerjee","userId":"02766170430590282737"}},"outputId":"bdc86480-b06b-4df5-fd22-5655f04a61d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version= 2.2.1+cu121\n","torchvision version= 0.17.1+cu121\n","CUDA available= False\n"]}]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    # CUDA Installation\n","    print('CUDA Version')\n","    !nvcc --version\n","    print()\n","\n","    # CUDNN Installation\n","    print(f'CUDNN Version: {torch.backends.cudnn.version()}')\n","    print(f'Number of CUDA Devices: {torch.cuda.device_count()}')\n","    print(f'Active CUDA Device: {torch.cuda.current_device()}')\n","    print(f'Available devices: {torch.cuda.device_count()}, Name: {torch.cuda.get_device_name(0)}')\n","    print(f'Current CUDA device: {torch.cuda.current_device()}')"],"metadata":{"id":"7qTtxktYJFaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QJ5HATUGD2X"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baiiDH6car9f"},"outputs":[],"source":["import os\n","import zipfile\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Unzip the dataset\n","zip_path = '/content/drive/My Drive/archive.zip'\n","extract_folder = '/content/dataset'\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_folder)\n","\n","# Setup for loading images\n","IMGSIZE = (128, 128)\n","CNAMES = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n","X_tr, y_tr, X_ts, y_ts = [], [], [], []\n","\n","# Load training images\n","path = extract_folder + '/seg_train/seg_train'\n","for label in CNAMES:\n","    label_path = os.path.join(path, label)\n","    for f in sorted([_ for _ in os.listdir(label_path) if _.lower().endswith('.jpg')]):\n","        img = cv2.imread(os.path.join(label_path, f))\n","        img_resized = cv2.resize(img, IMGSIZE)\n","        X_tr.append(img_resized)\n","        y_tr.append(CNAMES.index(label))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"prNwaZO5bL0E"},"outputs":[],"source":["# Convert lists to numpy arrays for better handling\n","X_tr = np.array(X_tr)\n","y_tr = np.array(y_tr)\n","\n","# Display a few images\n","plt.figure(figsize=(10, 10))\n","for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(cv2.cvtColor(X_tr[i], cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for displaying\n","    plt.title(CNAMES[y_tr[i]])\n","    plt.axis(\"off\")\n","plt.show()\n","\n","print(\"Number of color channels:\", X_tr.shape[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSDLts-YGZEu"},"outputs":[],"source":["X_tr = np.array(X_tr, dtype=np.float32)\n","\n","# Scale the pixel values to the range [0, 1]\n","X_tr /= 255.0\n","\n","print(\"Array shape:\", X_tr.shape)\n","print(\"Pixel range:\", X_tr.min(), X_tr.max())"]},{"cell_type":"markdown","source":["1. [60 pts] Create a convolutional neural network (CNN), train it on the testing portion of the dataset, and report its reclassification performance. 95% reclassification and 75% testing performance should be achievable without any hyperparameter tuning. (Hint: My model, which is similar to the model in module notebook, took around 10 minutes to train 10 epochs without a GPU)"],"metadata":{"id":"IUZQ87JSMwEY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","import numpy as np\n","\n","# Helper function to calculate output shape of each convolution layer\n","def findConv2dOutShape(H, W, conv, pool=2):\n","    kernel_size, stride, padding, dilation = conv.kernel_size, conv.stride, conv.padding, conv.dilation\n","    H = np.floor((H + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n","    W = np.floor((W + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n","    if pool:\n","        H, W = H / pool, W / pool\n","    return int(H), int(W)\n","\n","# Setup dataset loaders\n","def get_dataloader(img_size, batch_size, path):\n","    transform = transforms.Compose([\n","        transforms.Resize(img_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","    dataset = datasets.ImageFolder(root=path, transform=transform)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    return dataloader\n","\n","train_path = '/content/dataset/seg_train/seg_train'\n","test_path = '/content/dataset/seg_test/seg_test'\n","\n","train_loader = get_dataloader((128, 128), 64, train_path)\n","test_loader = get_dataloader((128, 128), 64, test_path)"],"metadata":{"id":"_xai6RFKY6RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","import numpy as np\n","\n","\n","# Simple CNN Model Definition\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)  # Output size changes for 3 input channels\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n","        self.fc1 = nn.Linear(32 * 32 * 32, 512)  # Adjusted for output size of conv2\n","        self.fc2 = nn.Linear(512, 6)  # Adjusted for 6 classes in Intel dataset\n","\n","    def forward(self, x):\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = x.view(-1, 32 * 32 * 32)  # Flatten\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Initialize model and training components\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = SimpleCNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"3WxZpTqGMyPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# Training the model\n","for epoch in range(10):\n","    model.train()\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"],"metadata":{"id":"r0rTA5cZTfWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Accuracy of the network on the test images: {accuracy}%')"],"metadata":{"id":"F9qM1oaOTmmZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the model to evaluation mode\n","model.eval()\n","\n","correct_train = 0\n","total_train = 0\n","\n","with torch.no_grad():\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","# Calculate and print the reclassification (training) accuracy\n","reclassification_accuracy = 100 * correct_train / total_train\n","print(f'Reclassification Accuracy on Training Data: {reclassification_accuracy:.2f}%')"],"metadata":{"id":"amuo8pdWTEGH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. [20 pts] Add regularization and/or drop-out features to your CNN. Report your model's best\n","performance. As the performance standard deviation decreases the model is deemed to be\n","more robust. Why?\n","\n","**Ans.**"],"metadata":{"id":"TsCptvxUTDkn"}},{"cell_type":"code","source":["class CNNwithRegDrop(nn.Module):\n","    def __init__(self):\n","        super(CNNwithRegDrop, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.dropout1 = nn.Dropout(0.25)  # Dropout after pooling\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n","        self.dropout2 = nn.Dropout(0.25)  # Dropout after second pooling\n","        self.fc1 = nn.Linear(32 * 32 * 32, 512)\n","        self.dropout3 = nn.Dropout(0.5)  # Dropout before final FC layer\n","        self.fc2 = nn.Linear(512, 6)\n","\n","    def forward(self, x):\n","        x = self.dropout1(self.pool(torch.relu(self.conv1(x))))\n","        x = self.dropout2(self.pool(torch.relu(self.conv2(x))))\n","        x = x.view(-1, 32 * 32 * 32)\n","        x = self.dropout3(torch.relu(self.fc1(x)))\n","        x = self.fc2(x)\n","        return x\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = CNNwithRegDrop().to(device)\n","criterion = nn.CrossEntropyLoss()\n","# Include L2 Regularization in the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)  # weight_decay is the L2 penalty"],"metadata":{"id":"T4AsP-IQM7_r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","epoch_accuracies = []\n","\n","# Training the model\n","for epoch in range(30):\n","    model.train()\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate the model after each epoch\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    epoch_accuracy = 100 * correct / total\n","    epoch_accuracies.append(epoch_accuracy)\n","    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {epoch_accuracy}%')\n","\n","# Calculate and print the standard deviation of accuracies\n","std_deviation = np.std(epoch_accuracies)\n","print(f'Standard Deviation of Accuracies over Training: {std_deviation:.2f}%')"],"metadata":{"id":"ihtijC7MKGO8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The implementation of regularization techniques like dropout and weight decay in a model contributes to enhanced robustness by preventing overfitting and promoting a more distributed representation of learning. This leads to a decrease in performance variability, indicating a more consistent and stable model across different datasets. The decreased standard deviation in performance metrics not only shows that the model is effectively handling diverse data inputs and learning genuine patterns but also reflects its stable learning process and robustness, increasing confidence in its deployment in real-world applications.\n","\n","\n","\n","3. [20 pts] Add batch normalization and early stopping features to the pipeline and demonstrate their effectiveness.\n","\n","**Ans.**\n","- Batch Normalization:"],"metadata":{"id":"ilJheK5nH5_D"}},{"cell_type":"code","source":["%%time\n","\n","class BatchCNN(nn.Module):\n","    def __init__(self):\n","        super(BatchCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n","        self.bn1 = nn.BatchNorm2d(16)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.dropout1 = nn.Dropout(0.25)\n","\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n","        self.bn2 = nn.BatchNorm2d(32)\n","        self.dropout2 = nn.Dropout(0.25)\n","\n","        self.fc1 = nn.Linear(32 * 32 * 32, 512)\n","        self.bn3 = nn.BatchNorm1d(512)\n","        self.dropout3 = nn.Dropout(0.5)\n","\n","        self.fc2 = nn.Linear(512, 6)\n","\n","    def forward(self, x):\n","        x = self.dropout1(self.pool(self.bn1(torch.relu(self.conv1(x)))))\n","        x = self.dropout2(self.pool(self.bn2(torch.relu(self.conv2(x)))))\n","        x = x.view(-1, 32 * 32 * 32)\n","        x = self.dropout3(self.bn3(torch.relu(self.fc1(x))))\n","        x = self.fc2(x)\n","        return x\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = BatchCNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n","\n","for epoch in range(30):\n","    model.train()\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n","\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Accuracy of the network on the test images: {accuracy}%')"],"metadata":{"id":"wqJfbfQNPTdh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Batch Normalization typically shows its effectiveness through faster convergence rates, allowing usage of higher learning rates reliably and making the network less sensitive to initialization."],"metadata":{"id":"YWOjV5jqPzif"}},{"cell_type":"code","source":["# Early stopping criteria\n","class EarlyStopping:\n","    def __init__(self, patience=5, verbose=False, delta=0):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.delta = delta\n","        self.best_score = None\n","        self.epochs_no_improve = 0\n","        self.early_stop = False\n","\n","    def __call__(self, val_loss, model):\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","        elif score < self.best_score + self.delta:\n","            self.epochs_no_improve += 1\n","            if self.verbose:\n","                print(f'EarlyStopping counter: {self.epochs_no_improve} out of {self.patience}')\n","            if self.epochs_no_improve >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.epochs_no_improve = 0\n","\n","\n","early_stopping = EarlyStopping(patience=5, verbose=True)\n","\n","for epoch in range(50):  # More epochs for demonstration\n","    model.train()\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation phase\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:  # Ensure a validation loader is defined similarly to train_loader\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            val_loss += criterion(outputs, labels).item()\n","\n","    val_loss /= len(val_loader)\n","    print(f'Epoch {epoch+1}, Validation Loss: {val_loss}')\n","\n","    # Call early stopping\n","    early_stopping(val_loss, model)\n","    if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break"],"metadata":{"id":"xsXyzbkTRtUF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Early Stopping is demonstrated effective if it stops training once the model begins to overfit (validation performance starts to degrade or stops improving)."],"metadata":{"id":"cQtR38HNRyaR"}}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}