{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcd8795",
   "metadata": {},
   "source": [
    "### 1. \n",
    "[20 pts] At a high level (i.e., without entering into mathematical details), please describe,\n",
    "compare, and contrast the following classifiers:\n",
    "-  Perceptron (textbook's version)\n",
    "-  SVM\n",
    "-  Decision Tree\n",
    "- Random Forest (you have to research a bit about this classifier)\n",
    "\n",
    "Some comparison criterion can be:\n",
    "-  Speed?\n",
    "-  Strength?\n",
    "-  Robustness?\n",
    "-  The feature type that the classifier naturally uses (e.g. relying on distance means that numerical features are naturally used)\n",
    "-  Is it statistical?\n",
    "-  Does the method solve an optimization problem? If yes, what is the cost function?\n",
    "Which one will be the first that you would try on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24416af1",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    "1. **Perceptron:**\n",
    "- **Speed**: Generally fast due to its simplicity.\n",
    "- **Strength**: Good for linearly separable problems but struggles with non-linear data.\n",
    "- **Robustness**: Sensitive to noisy data and outliers.\n",
    "- **Feature Type**: Works best with numerical features as it operates based on linear combinations of input features.\n",
    "- **Statistical**: Not particularly, more geometric in nature.\n",
    "- **Optimization Problem**: Yes, it tries to find a separating hyperplane by minimizing classification errors.\n",
    "\n",
    "2. **SVM:** [1]\n",
    "- **Speed**: Can be slow, especially with large datasets and when using kernel tricks.\n",
    "- **Strength**: Very effective for high-dimensional spaces and for cases where the number of dimensions exceeds the number of samples.\n",
    "- **Robustness**: Quite robust against overfitting, especially in high-dimensional space. Kernel SVMs can handle non-linear data well.\n",
    "- **Feature Type**: Primarily numerical; kernel tricks allow it to handle non-linear relationships.\n",
    "- **Statistical**: Yes, based on statistical learning theory.\n",
    "- **Optimization Problem**: Yes, it solves a convex optimization problem to maximize the margin between classes.\n",
    "\n",
    "3. **Decision Tree:** [2]\n",
    "- **Speed**: Fast for training, but predictions can be slower with very deep trees.\n",
    "- **Strength**: Can handle both numerical and categorical data well. Easy to interpret and understand.\n",
    "- **Robustness**: Prone to overfitting, especially with complex trees.\n",
    "- **Feature Type**: Can naturally handle a mix of numerical and categorical features without needing dummy variables.\n",
    "- **Statistical**: Yes, it uses statistical measures (like information gain or Gini impurity) to split the data.\n",
    "\n",
    "4. **Random Forest:** [2]\n",
    "- **Speed**: Slower than Decision Trees due to the ensemble approach but can be parallelized.\n",
    "- **Strength**: Very strong classifier due to the ensemble of decision trees which reduces overfitting and improves generalization.\n",
    "- **Robustness**: More robust than a single decision tree, less sensitive to noisy data and outliers.\n",
    "- **Feature Type**: Like decision trees, it can handle both numerical and categorical features effectively.\n",
    "- **Statistical**: Yes, it builds upon the statistical approach of decision trees and adds methods like bootstrapping and aggregation to improve performance.\n",
    "\n",
    "#### Which to Try First?\n",
    "The choice of classifier to try first on a new dataset might depend on the dataset's size, feature types, and the problem's complexity. If the dataset is high-dimensional and linearly separable, a Perceptron or SVM might be the first choice. For a dataset with mixed feature types and potential non-linear relationships, a Decision Tree or Random Forest might be more appropriate due to their versatility and ability to handle complexity and avoid overfitting through ensemble methods.\n",
    "\n",
    "Ultimately, the best approach is experimenting with multiple models and validate to compare their performance systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08549f",
   "metadata": {},
   "source": [
    "### 2. \n",
    "[20 pts] Define the following feature types and give example values from a dataset. You\n",
    "can pull examples from an existing dataset (like the Iris dataset) or you could write out a\n",
    "dataset yourself. (Hint:** In order to give examples for each feature type, you will probably\n",
    "have to use more than one dataset.)\n",
    "\n",
    "**Ans:**\n",
    "- **Numerical:** Numerical features represent quantitative measurements and can be either discrete or continuous. Example from the Iris dataset: The 'sepal length' feature is a  quantifies the length of the sepal in centimeters.\n",
    "\n",
    "- **Nominal:** categorical features, represent categories or groups that do not have a specific order or ranking. Hypothetical example: A dataset of cars might have a 'color' feature that is nominal. Example values could be Red, Blue, Green, etc.\n",
    "\n",
    "- **Date:** Date features represent dates and/or times, often used to mark events or records. Hypothetical example: A dataset of library book checkouts might include a 'checkout date' feature. Example values could be 2023-01-15,  etc., in the format YYYY-MM-DD.\n",
    "\n",
    "- **Text:** Text features contain textual data, often unstructured, such as sentences, paragraphs, or documents. Hypothetical example: A dataset of customer reviews might include a 'review' feature. Example values could be \"Excellent service, will come again!\", \"Product not as described, very disappointed.\", etc.\n",
    "\n",
    "- **Image:** Image features consist of data in the form of images, which can be used for tasks like image classification, object detection, etc. Hypothetical example: A dataset for a facial recognition system might include an 'image' feature, where each record is an image file of a person's face. Example values  imagine filenames like person1.jpg, person2.jpg, etc\n",
    "\n",
    "- **Dependent variable:** The variable that we are trying to predict or explain. Example from the Iris dataset: The 'species' feature is the dependent variable, indicating the species of Iris plant (Iris setosa, Iris virginica, Iris versicolor) for classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca2f76",
   "metadata": {},
   "source": [
    "### 3. \n",
    "[20 pts] Using online resources, research and find other classifier performance metrics\n",
    "which are also as common as the accuracy metric. Provide the mathematical equations\n",
    "for them and explain in your own words the meaning of the different metrics you found.\n",
    "Note that providing mathematical equations might involve defining some more fundamental\n",
    "terms, e.g. you should define “False Positive,” if you answer with a metric that builds on\n",
    "that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b852d",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "1. **Precision**:\n",
    "Precision measures the accuracy of positive predictions. It is the ratio of correctly predicted positive observations to the total predicted positives. The formula is given by:\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "Where:\n",
    "- \\(TP\\) (True Positives) is the number of positive instances correctly identified by the model.\n",
    "- \\(FP\\) (False Positives) is the number of negative instances incorrectly labeled as positive by the model.\n",
    "\n",
    "2. **Recall (Sensitivity or True Positive Rate)**:\n",
    "Recall is the ratio of correctly predicted positive observations to all observations in the actual class. It is defined as:\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "Where:\n",
    "- \\(FN\\) (False Negatives) is the number of positive instances incorrectly labeled as negative by the model.\n",
    "\n",
    "3. **F1 Score** [3]:\n",
    "The F1 Score is the harmonic mean of Precision and Recall, providing a single metric to assess a model's performance when you need a balance between precision and recall. The formula is:\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "4. **AUC-ROC** [3]:\n",
    "The ROC (Receiver Operating Characteristics) curve visualizes the model's performance by plotting the true positive rate against the false positive rate at various classification thresholds. The AUC represents the total area beneath this plot, serving as an indicator of the model's performance across all potential classification thresholds. AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes.  ROC curves are particularly useful for assessing the accuracy of models, making them ideal for evaluating model performance in scenarios where the data distribution is balanced.\n",
    "\n",
    "- True Positive Rate (TPR): \n",
    "$$\n",
    "\\text{TPR} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "- False Positive Rate (FPR): \n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "Where \\(TN\\) (True Negatives) is the number of negative instances correctly identified by the model.\n",
    "\n",
    "5. **Log Loss (Cross-Entropy Loss)** [4]:\n",
    "Log Loss quantifies the accuracy of a classifier by penalising false classifications. It is defined for a binary classifier as:\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n",
    "$$\n",
    "Where:\n",
    "- \\(N\\) is the number of samples.\n",
    "- \\(y_i\\) is the actual label of instance \\(i\\), which can be 0 or 1.\n",
    "- \\(p_i\\) is the predicted probability that instance \\(i\\) is of class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052fb24",
   "metadata": {},
   "source": [
    "### 4 \n",
    "Implement a correlation program from scratch to look at the correlations between the features of Admission_Predict.csv dataset file. (This Graduate Admission dataset, with 9 features and 500 data points, is not provided on Canvas; you have to download it from Kaggle by following the instructions in the module Jupyter notebook.) \n",
    "Remember, you are not allowed to usednumpyfunctionssuch asmean(),stdev(),cov(),etc.\n",
    "You may use DataFrame.corr() only to verify the correctness of your from-scratch matrix.\n",
    "  \n",
    "Display the correlation matrix where each row and column are the features. (Hint: this should be an 8 by 8 matrix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca85e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows=500, M columns=9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Locate and load the data file\n",
    "df = pd.read_csv('../../Desktop/APML/Datasets/Admission_Predict_Ver1.1.csv')\n",
    "\n",
    "# Check dimensions\n",
    "print(f'N rows={len(df)}, M columns={len(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5517bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Serial No. GRE Score TOEFL Score University Rating  \\\n",
      "Serial No.               1.0 -0.103839   -0.141696         -0.067641   \n",
      "GRE Score          -0.103839       1.0      0.8272          0.635376   \n",
      "TOEFL Score        -0.141696    0.8272         1.0          0.649799   \n",
      "University Rating  -0.067641  0.635376    0.649799               1.0   \n",
      "SOP                -0.137352  0.613498     0.64441          0.728024   \n",
      "LOR                -0.003694  0.524679    0.541563          0.608651   \n",
      "CGPA               -0.074289  0.825878    0.810574          0.705254   \n",
      "Research           -0.005332  0.563398    0.467012          0.427047   \n",
      "Chance of Admit     0.008505  0.810351    0.792228          0.690132   \n",
      "\n",
      "                        SOP      LOR       CGPA  Research Chance of Admit   \n",
      "Serial No.        -0.137352 -0.003694 -0.074289 -0.005332         0.008505  \n",
      "GRE Score          0.613498  0.524679  0.825878  0.563398         0.810351  \n",
      "TOEFL Score         0.64441  0.541563  0.810574  0.467012         0.792228  \n",
      "University Rating  0.728024  0.608651  0.705254  0.427047         0.690132  \n",
      "SOP                     1.0  0.663707  0.712154  0.408116         0.684137  \n",
      "LOR                0.663707       1.0  0.637469  0.372526         0.645365  \n",
      "CGPA               0.712154  0.637469       1.0  0.501311         0.882413  \n",
      "Research           0.408116  0.372526  0.501311       1.0         0.545871  \n",
      "Chance of Admit    0.684137  0.645365  0.882413  0.545871              1.0  \n"
     ]
    }
   ],
   "source": [
    "def mean(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "def stddev(values, mean):\n",
    "    return (sum(([(x - mean)** 2 for x in values])) / (len(values) - 1)) ** 0.5\n",
    "\n",
    "def covariance(x, x_mean, y, y_mean):\n",
    "    covariance = 0.0\n",
    "    for i in range(len(x)):\n",
    "        covariance += (x[i] - x_mean) * (y[i]  - y_mean)\n",
    "    return covariance / (len(x) - 1)\n",
    "\n",
    "# Function to calculate Pearson's correlation coefficient\n",
    "def pearson_correlation(x, y):\n",
    "    mean_x, mean_y = mean(x), mean(y)\n",
    "    covar = covariance(x, mean_x, y, mean_y)\n",
    "    stddev_x, stddev_y  = stddev(x, mean_x), stddev(y, mean_y)\n",
    "    return covar / (stddev_x * stddev_y)\n",
    "\n",
    "# Initialize an empty DataFrame for the correlation matrix\n",
    "# features = df.columns[:-1]  # Exclude 'Chance of Admit' for the matrix\n",
    "features = df.columns  \n",
    "correlation_matrix = pd.DataFrame(index=features, columns=features)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "for col1 in features:\n",
    "    for col2 in features:\n",
    "        correlation_matrix.loc[col1, col2] = pearson_correlation(df[col1], df[col2])\n",
    "\n",
    "# Print the DataFrame directly to display it as a table\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000e18f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Serial No.  GRE Score  TOEFL Score  University Rating  \\\n",
      "Serial No.           1.000000  -0.103839    -0.141696          -0.067641   \n",
      "GRE Score           -0.103839   1.000000     0.827200           0.635376   \n",
      "TOEFL Score         -0.141696   0.827200     1.000000           0.649799   \n",
      "University Rating   -0.067641   0.635376     0.649799           1.000000   \n",
      "SOP                 -0.137352   0.613498     0.644410           0.728024   \n",
      "LOR                 -0.003694   0.524679     0.541563           0.608651   \n",
      "CGPA                -0.074289   0.825878     0.810574           0.705254   \n",
      "Research            -0.005332   0.563398     0.467012           0.427047   \n",
      "Chance of Admit      0.008505   0.810351     0.792228           0.690132   \n",
      "\n",
      "                        SOP      LOR       CGPA  Research  Chance of Admit   \n",
      "Serial No.        -0.137352 -0.003694 -0.074289 -0.005332          0.008505  \n",
      "GRE Score          0.613498  0.524679  0.825878  0.563398          0.810351  \n",
      "TOEFL Score        0.644410  0.541563  0.810574  0.467012          0.792228  \n",
      "University Rating  0.728024  0.608651  0.705254  0.427047          0.690132  \n",
      "SOP                1.000000  0.663707  0.712154  0.408116          0.684137  \n",
      "LOR                0.663707  1.000000  0.637469  0.372526          0.645365  \n",
      "CGPA               0.712154  0.637469  1.000000  0.501311          0.882413  \n",
      "Research           0.408116  0.372526  0.501311  1.000000          0.545871  \n",
      "Chance of Admit    0.684137  0.645365  0.882413  0.545871          1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation matrix using pandas\n",
    "correlation_matrix_pandas = df.corr()\n",
    "\n",
    "# Display the correlation matrix from pandas for comparison\n",
    "print(correlation_matrix_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a311e",
   "metadata": {},
   "source": [
    "- • Should we use 'Serial no'? Why or why not?\n",
    "\n",
    "**Ans:** No, we should not use 'Serial No.' as a feature. Typically a unique identifier for each entry in a dataset and does not hold any intrinsic predictive value regarding the outcomes. \n",
    "\n",
    "- • Observe that the diagonal of this matrix should have all 1's; why is this?\n",
    "\n",
    "**Ans:** The diagonal of a correlation matrix has all 1's because it represents the correlation of each variable with itself. Since the correlation measures the strength and direction of a linear relationship between two variables, the correlation of a variable with itself is always perfect, hence a coefficient of 1.\n",
    "\n",
    "- • Since the last column can be used as the target (dependent) variable, what do you think about the correlations between all the variables?\n",
    "\n",
    "**Ans:** When looking at the correlations with the target variable 'Chance of Admit', variables with higher absolute values of the correlation coefficient are more linearly related, positively or negatively, to the chances of admission.\n",
    "- • Which variable should be the most important to try to predict 'Chance of Admit'?\n",
    "\n",
    "**Ans:** Based on the correlation matrix, the variable 'CGPA' has the highest correlation with 'Chance of Admit' (0.882413). This suggests that 'CGPA' likely would be the most important predictor in a predictive model for admission chances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6fc3a",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. DeepAI. (n.d.). Support Vector Machine Definition. Retrieved from https://deepai.org/machine-learning-glossary-and-terms/support-vector-machine\n",
    "2. IBM. (n.d.). Random Forest. Retrieved from https://www.ibm.com/topics/random-forest \n",
    "3. Erickson, B. J., & Kitamura, F. (2021). Magician’s Corner: 9. Performance Metrics for Machine Learning Models. *Radiology: Artificial Intelligence*, *3*(3). [https://doi.org/10.1148/ryai.2021200126](https://doi.org/10.1148/ryai.2021200126)\n",
    "4. Roberts, A. (2023, January 30). Binary Cross Entropy: Where To Use Log Loss In Model Monitoring. Arize Blog. https://arize.com/blog-course/binary-cross-entropy-log-loss/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584ab49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
